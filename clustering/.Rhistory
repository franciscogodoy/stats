m = cbind(c(1,2,3),c(4,7,13),1)
m
eigen(m)
m = cbind(c(1,2,3),c(1,3,1),1)
eigen(m)
m
data(iris)
iris
mean(iris)
mean(iris[,1:4])
summary(iris)
iris[,1:4] = (iris[,1:4] - mean(iris[,1:4]))/sd(iris[,1:4])
iris
mean(iris[,1:4])
iris
data(iris)
iris
summary(iris)
m = cbind(c(1,2,3),c(1,3,1),1)
eigen(m)
m
ir <- rbind(iris3[,,1],iris3[,,2],iris3[,,3])
iris3
iris3
ir <- rbind(iris3[,,1],iris3[,,2],iris3[,,3])
ir
iris
targets <- class.ind( c(rep("s", 50), rep("c", 50), rep("v", 50)) )
targets <- class.ind( c(rep("s", 50), rep("c", 50), rep("v", 50)) )
class.ind <- function(cl)
{
n <- length(cl)
cl <- as.factor(cl)
x <- matrix(0, n, length(levels(cl)) )
x[(1:n) + n*(unclass(cl)-1)] <- 1
dimnames(x) <- list(names(cl), levels(cl))
x
}
targets <- class.ind( c(rep("s", 50), rep("c", 50), rep("v", 50)) )
targets
ird <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
species = factor(c(rep("s",50), rep("c", 50), rep("v", 50))))
ird
samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
samp
length(samp)
class(iris)
class(iris3)
iris3
ir.nn2 <- nnet(species ~ ., data = ird, subset = samp, size = 2, rang = 0.1,
decay = 5e-4, maxit = 200)
library(nnet)
ir.nn2 <- nnet(species ~ ., data = ird, subset = samp, size = 2, rang = 0.1,
decay = 5e-4, maxit = 200)
table(ird$species[-samp], predict(ir.nn2, ird[-samp,], type = "class"))
ir <- rbind(iris3[,,1],iris3[,,2],iris3[,,3])
targets <- class.ind( c(rep("s", 50), rep("c", 50), rep("v", 50)) )
samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
ir1 <- nnet(ir[samp,], targets[samp,], size = 2, rang = 0.1,
decay = 5e-4, maxit = 200)
test.cl <- function(true, pred) {
true <- max.col(true)
cres <- max.col(pred)
table(true, cres)
}
test.cl(targets[-samp,], predict(ir1, ir[-samp,]))
coef.nnet (ir.nn2)
iris3[,,1]
ird$species[-samp]
ird$species[samp]
ird$species[samp]
ird$species
ird$species[samp]
samp
-samp
ird$species[-samp]
ird$species[samp]
samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
samp
samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
samp
samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
train_sample =  c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
ird <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
species = factor(c(rep("s",50), rep("c", 50), rep("v", 50))))
train_sample =  c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
test_sample =  c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
ir.nn2 <- nnet(species ~ ., data = ird, subset = train_sample, size = 2, rang = 0.1,
decay = 5e-4, maxit = 200)
table(ird$species[test_sample], predict(ir.nn2, ird[test_sample,], type = "class"))
ird$species[test_sample]
predict(ir.nn2, ird[test_sample,]
)
ird <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]), 
species = factor(c(rep("s",50), rep("c", 50), rep("v", 50))))
train_sample =  c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
test_sample  =  c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
ir.nn2 <- nnet(species ~ ., data = ird, subset = train_sample, size = 2, rang = 0.1,
decay = 5e-4, maxit = 200)
ird$species[~test_sample]
ird$species[train_sample]
ird$species[-train_sample]
ird$species[test_sample]
?sample
train_sample =  c(sample(1:150,150);
train_sample =  c(sample(1:150,150));
train_example
train_sample =  c(sample(1:150,150))
train_example
train_sample
samp  =  c(sample(1:50,50), sample(51:100,50), sample(101:150,50));
data  =  data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]), 
species = factor(c(rep("s",50), rep("c", 50), rep("v", 50))));
train_sample = 1:2:size(data,1);
test_sample  = 2:2:size(data,1);
samp  =  c(sample(1:50,50), sample(51:100,50), sample(101:150,50));
data  =  data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]), 
species = factor(c(rep("s",50), rep("c", 50), rep("v", 50))));
train_sample = 1:2:150;
test_sample  = 2:2:150;
train_example
train_sample
?rep
?seq
train_sample = seq(1,2,150);
train_sample
train_sample = seq(1,150,2);
seq(1,150,2)
seq(2,150,2)
samp  =  c(sample(1:50,50), sample(51:100,50), sample(101:150,50));
data  =  data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]), 
species = factor(c(rep("s",50), rep("c", 50), rep("v", 50))));
train_sample = seq(1,150,2);
test_sample  = seq(2,150,2);
ir.nn <- nnet(species ~ ., data = ird, subset = train_sample, size = 2, rang = 0.1,
decay = 5e-4, maxit = 200)
table(ird$species[test_sample], predict(ir.nn2, ird[test_sample,], type = "class"))
ls -la
data(iris);
data
mydata <- na.omit(data);
#standardize variables
mydata <- scale(mydata);
# K-means clustering requires the analyst to specify the number of clusters
 # A plot of the within groups sum of squares by number of clusters extracted 
# can help determine the appropriate number of clusters. 
# The analyst looks for a bend in the plot similar to a scree test in factor analysis.
# Determine number of clusters
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
mydata <- scale(data);
data
mydata <- na.omit(data);
mydata <- scale(data[,1:4]);
mydata
names(mydata)
mydata
mydata <- na.omit(data);
#standardize variables
mydata <- scale(data[,1:4]);
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
fit <- kmeans(mydata, 3);
fit
fit$centers
fit$cluster
data
head(data)
table(data$species, fit$cluster);
table(fit$cluster, data$species);
cluster
fit$cluster
?kmeans
aggregate(mydata,by=list(fit$cluster),FUN=mean)
fit$centers
mydata <- data.frame(mydata, fit$cluster)
mydata
table(fit$cluster, data$species);
head(data)
head(mydata)
x <- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2));
colnames(x) <- c("x", "y");
(cl <- kmeans(x, 2));
plot(x, col = cl$cluster);
points(cl$centers, col = 1:2, pch = 8, cex=2);
utils:::menuInstallPkgs()
libraray(fpt)
library(fpt)
library(fpc)
?pamk
set.seed(20000)
  face <- rFace(50,dMoNo=2,dNoEy=0,p=2)
  pk <- pamk(face,krange=1:5,criterion="asw",critout=TRUE)
  pk <- pamk(face,krange=1:5,criterion="ch",critout=TRUE)
set.seed(4634)
face <- rFace(600,dMoNo=2,dNoEy=0)
grface <- as.integer(attr(face,"grouping"))
adcf <- adcoord(face,grface==2)
adcf2 <- adcoord(face,grface==4)
plot(adcf$proj,col=1+(grface==2))
set.seed(4634)
face <- rFace(600,dMoNo=2,dNoEy=0)
grface <- as.integer(attr(face,"grouping"))
awcf <- awcoord(face,grface==1)
# awcf2 <- ancoord(face,grface==1, method="mcd")
plot(awcf$proj,col=1+(grface==1))
set.seed(4634)
face <- rFace(600,dMoNo=2,dNoEy=0)
grface <- as.integer(attr(face,"grouping"))
bcf2 <- batcoord(face,grface==2)
plot(bcf2$proj,col=1+(grface==2))
bcfv2 <- batcoord(face,grface==2,dom="variance")
plot(bcfv2$proj,col=1+(grface==2))
bcfvv2 <- batvarcoord(face,grface==2)
plot(bcfvv2$proj,col=1+(grface==2))
set.seed(4634)
face <- rFace(600,dMoNo=2,dNoEy=0)
grface <- as.integer(attr(face,"grouping"))
bcf2 <- batcoord(face,grface==2)
plot(bcf2$proj,col=1+(grface==2))
bcfv2 <- batcoord(face,grface==2,dom="variance")
plot(bcfv2$proj,col=1+(grface==2))
bcfvv2 <- batvarcoord(face,grface==2)
plot(bcfvv2$proj,col=1+(grface==2))d <- dist(mydata, method = "euclidean") # distance matrix
data(iris);
# listwise deletion of missing
data <- na.omit(data);
#standardize variables
mydata <- scale(data[,1:4]);
# Ward Hierarchical Clustering
d <- dist(mydata, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward") 
d
class(d)
names(d)
fit <- hclust(d, method="ward") 
plot(fit) 
groups <- cutree(fit, k=5) 
# cut tree into 5 clusters
rect.hclust(fit, k=5, border="red")
plot(fit) 
groups <- cutree(fit, k=5); 
groups
groupsgroups <- cutree(fit, k=3); 
groups <- cutree(fit, k=3); 
groups
table(groups, data$species)
groups <- cutree(fit, k=3); 
rect.hclust(fit, k=3, border="red");
# Ward Hierarchical Clustering
d <- dist(mydata, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward") 
plot(fit) # display dendogram
groups <- cutree(fit, k=5) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters 
rect.hclust(fit, k=5, border="red")
mydata
mydata = t(mydata)
mydata
mydata = t(mydata)library(pvclust);
library(pvclust);
packages.install('pvclust');
package.install('pvclust');
utils:::menuInstallPkgs()
install('pvclust');
library(pvclust);
fit <- pvclust(mydata, method.hclust="ward", method.dist="euclidean");
mydata <- scale(data[,1:4]);
fit <- kmeans(mydata, 3);
library(cluster); 
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0);
library(fpc);
plotcluster(mydata, fit$cluster);
?kmeans
?dist
mydata
data
# distance matrix
d <- dist(mydata, method = "maximum");
fit <- hclust(d, method="ward"); 
groups <- cutree(fit, k=3); 
fit
groups
library(cluster); 
clusplot(mydata, groups, color=TRUE, shade=TRUE, labels=2, lines=0);
mydata = data[,1:4];
d <- dist(mydata, method = "maximum");
fit <- hclust(d, method="ward"); 
groups <- cutree(fit, k=3); 
library(cluster); 
clusplot(mydata, groups, color=TRUE, shade=TRUE, labels=2, lines=0);
clusplot(mydata, groups, color=TRUE, shade=TRUE, labels=1, lines=0);
clusplot(mydata, groups, color=TRUE, shade=FALSE, labels=1, lines=0);
# Cluster Plot against 1st 2 principal components
# vary parameters for most readable graph
library(cluster); 
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0);
# Centroid Plot against 1st 2 discriminant functions
library(fpc);
plotcluster(mydata, fit$cluster);
mydata <- scale(data[,1:4]);
fit <- kmeans(mydata, 3);
# Cluster Plot against 1st 2 principal components
# vary parameters for most readable graph
library(cluster); 
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0);
# Centroid Plot against 1st 2 discriminant functions
library(fpc);
plotcluster(mydata, fit$cluster);
data(iris);
# listwise deletion of missing
data <- na.omit(data);
#standardize variables
mydata <- scale(data[,1:4]);
# K-means clustering requires the analyst to specify the number of clusters
 # A plot of the within groups sum of squares by number of clusters extracted 
# can help determine the appropriate number of clusters. 
# The analyst looks for a bend in the plot similar to a scree test in factor analysis.
# Determine number of clusters
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var));
for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss);
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares");
# K-Means Cluster Analysis. 3 cluster solution
fit <- kmeans(mydata, 3);
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var));
for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss);
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares");
fit <- kmeans(mydata, 3);
aggregate(mydata,by=list(fit$cluster),FUN=mean)
mydata <- data.frame(mydata, fit$cluster)
mydata
library(cluster); 
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0);
library(fpc);
plotcluster(mydata, fit$cluster);
?mvrnorm
Sigma <- matrix(c(10,3,3,2),2,2)
Sigma
 rep(0, 2)
Mu2 = c(-3,-5);
Sigma2 <- matrix(c(1,0,0,1),2,2);
Sigma2
Mu1 = c(1,2);
Sigma1 <- matrix(c(2,0,0,.5),2,2);
Mu2 = c(-3,-5);
Sigma2 <- matrix(c(1,0,0,1),2,2);
Sigma1
Sigma2
Mu1
Mu2
mvrnorm(n=1000, rep(0, 2), Sigma);
mvrnorm(n=1000, Mu1), Sigma1);
mvrnorm(n=1000, Mu1, Sigma1);
Mu1 = c(1,2);
Sigma1 <- matrix(c(2,0,0,.5),2,2);
Mu2 = c(-3,-5);
Sigma2 <- matrix(c(1,0,0,1),2,2);
X = rbind(mvrnorm(n=1000, Mu1, Sigma1), mvrnorm(n=1000, Mu2, Sigma2));
X
install.packages("mclust");
library(mclust);
fit <- Mclust(X);
plot(fit, X);
print(fit); 
fit
names(fit)
fit$classification
fit$G
fit$parameters
data(iris);
mydata = data[,1:4];
fit2 <- Mclust(mydata)
plot(fit2, mydata) # plot results 
print(fit2$classification);
print(fit2$parameters);
